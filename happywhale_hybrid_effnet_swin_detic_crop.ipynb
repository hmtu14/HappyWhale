{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hmtu14/HappyWhale/blob/main/happywhale_hybrid_effnet_swin_detic_crop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uB5kDl5ylBnB"
      },
      "source": [
        "# Intro\n",
        "\n",
        "The goal of this notebook is to extend upon the work of [MANOJ PRABHAKAR](https://www.kaggle.com/manojprabhaakr) in the [EFFNET B6 WHALE COMP](https://www.kaggle.com/manojprabhaakr/effnet-b6-whale-comp), utilising the [Detic bounding boxes](https://www.kaggle.com/c/happy-whale-and-dolphin/discussion/305503) to precrop the images.\n",
        "\n",
        "I have created a new set of TFRecords which includes the bounding boxes in [this](https://www.kaggle.com/lextoumbourou/happywhale-tfrecords-with-bounding-boxes) repo.\n",
        "\n",
        "At the time of creation, this notebook achieves a top 100 score. Given there's still 2 months left in the competition and that this data is all publically available, I felt it to be reasonable to release."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBOuPkvqlBnD",
        "outputId": "325d1656-16a9-4284-923c-de334d275e35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on TPU  grpc://10.60.103.18:8470\n",
            "INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.60.103.18:8470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.60.103.18:8470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "REPLICAS:  8\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "try:\n",
        "    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n",
        "    # set: this is always the case on Kaggle.\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "    print('Running on TPU ', tpu.master())\n",
        "except ValueError:\n",
        "    tpu = None\n",
        "\n",
        "if tpu:\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.TPUStrategy(tpu)\n",
        "else:\n",
        "    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
        "    strategy = tf.distribute.get_strategy()\n",
        "\n",
        "AUTO = tf.data.experimental.AUTOTUNE\n",
        "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install tfimm\n",
        "# !pip install -q efficientnet\n",
        "# !pip install tensorflow_addons\n",
        "# !pip install timm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tAonROR65_go",
        "outputId": "f722da15-2e9d-49f2-dda7-5db8fca72ae0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tfimm in /usr/local/lib/python3.7/dist-packages (0.2.5)\n",
            "Requirement already satisfied: markdown<4.0.0,>=3.3.6 in /usr/local/lib/python3.7/dist-packages (from tfimm) (3.3.6)\n",
            "Requirement already satisfied: tensorflow<3.0,>=2.5 in /usr/local/lib/python3.7/dist-packages (from tfimm) (2.8.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tfimm) (1.21.5)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown<4.0.0,>=3.3.6->tfimm) (4.11.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown<4.0.0,>=3.3.6->tfimm) (3.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown<4.0.0,>=3.3.6->tfimm) (3.10.0.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow<3.0,>=2.5->tfimm) (57.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3.0,>=2.5->tfimm) (0.2.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3.0,>=2.5->tfimm) (0.5.3)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3.0,>=2.5->tfimm) (1.1.2)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3.0,>=2.5->tfimm) (3.1.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3.0,>=2.5->tfimm) (1.44.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3.0,>=2.5->tfimm) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3.0,>=2.5->tfimm) (2.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3.0,>=2.5->tfimm) (2.8.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3.0,>=2.5->tfimm) (3.3.0)\n",
            "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3.0,>=2.5->tfimm) (2.8.0.dev2021122109)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3.0,>=2.5->tfimm) (1.0.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3.0,>=2.5->tfimm) (1.13.3)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3.0,>=2.5->tfimm) (3.17.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3.0,>=2.5->tfimm) (1.15.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3.0,>=2.5->tfimm) (1.1.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3.0,>=2.5->tfimm) (0.24.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3.0,>=2.5->tfimm) (13.0.0)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3.0,>=2.5->tfimm) (2.8.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow<3.0,>=2.5->tfimm) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow<3.0,>=2.5->tfimm) (1.5.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<3.0,>=2.5->tfimm) (0.4.6)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<3.0,>=2.5->tfimm) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<3.0,>=2.5->tfimm) (1.8.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<3.0,>=2.5->tfimm) (1.35.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<3.0,>=2.5->tfimm) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<3.0,>=2.5->tfimm) (0.6.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<3.0,>=2.5->tfimm) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<3.0,>=2.5->tfimm) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<3.0,>=2.5->tfimm) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow<3.0,>=2.5->tfimm) (1.3.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<3.0,>=2.5->tfimm) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<3.0,>=2.5->tfimm) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<3.0,>=2.5->tfimm) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<3.0,>=2.5->tfimm) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<3.0,>=2.5->tfimm) (2021.10.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow<3.0,>=2.5->tfimm) (3.2.0)\n",
            "Requirement already satisfied: tensorflow_addons in /usr/local/lib/python3.7/dist-packages (0.16.1)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow_addons) (2.7.1)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.7/dist-packages (0.5.4)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from timm) (1.10.0+cu111)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm) (0.11.1+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->timm) (3.10.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (1.21.5)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iJ1rGiN9lBnE"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import math\n",
        "import tensorflow as tf\n",
        "import efficientnet.tfkeras as efn\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "from tensorflow.keras import backend as K\n",
        "import tensorflow_addons as tfa\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import json\n",
        "import tensorflow_hub as tfhub\n",
        "from datetime import datetime\n",
        "import sys\n",
        "# !git clone https://github.com/rishigami/Swin-Transformer-TF.git\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !git clone https://github.com/rishigami/Swin-Transformer-TF.git\n"
      ],
      "metadata": {
        "id": "TIG7fmjeosI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rne3CS9QEkXo"
      },
      "outputs": [],
      "source": [
        "sys.path.append('/content/Swin-Transformer-TF/')\n",
        "from swintransformer import SwinTransformer\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tfimm"
      ],
      "metadata": {
        "id": "Tk2T4pRL6N_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# swin_obj_tfimm = tfimm.create_model(\"swin_base_patch4_window7_224\", pretrained=True, include_top=False)\n",
        "# swin_obj_lib = SwinTransformer(\"swin_base_224\", include_top=False, pretrained=True, use_tpu=True)"
      ],
      "metadata": {
        "id": "AQK3Uozm6Lnz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EFNS = [efn.EfficientNetB0, efn.EfficientNetB1, efn.EfficientNetB2, efn.EfficientNetB3, \n",
        "#         efn.EfficientNetB4, efn.EfficientNetB5, efn.EfficientNetB6, efn.EfficientNetB7]\n",
        "# eff_obj = EFNS[3](weights = 'noisy-student', include_top = False, input_shape = [config.IMAGE_SIZE, config.IMAGE_SIZE, 3])\n"
      ],
      "metadata": {
        "id": "JvWO8KON6oGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# eff_obj.summary()"
      ],
      "metadata": {
        "id": "lT7w8dZ87p7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sample = np.random.rand(config.BATCH_SIZE, config.IMAGE_SIZE, config.IMAGE_SIZE, 3)"
      ],
      "metadata": {
        "id": "g7x93nkjvBXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# eff_obj.get_layer(config.EFF_LAST_LAYER).output"
      ],
      "metadata": {
        "id": "e701YEWVXro9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1Ui4mwplBnE"
      },
      "source": [
        "## Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYOO7crGlBnF",
        "outputId": "319a2c71-b9e9-4947-de9d-779bc9b0e1bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20220309-184836\n"
          ]
        }
      ],
      "source": [
        "save_dir = '.'\n",
        "#EXPERIMENT = 0\n",
        "#EXPERIMENT = 1\n",
        "#EXPERIMENT = 2\n",
        "EXPERIMENT = 3\n",
        "#EXPERIMENT = 4\n",
        "\n",
        "run_ts = datetime.now().strftime('%Y%m%d-%H%M%S')\n",
        "print(run_ts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xaKpw9qOlBnF"
      },
      "outputs": [],
      "source": [
        "# GCS_DS_PATH=KaggleDatasets().get_gcs_path('happywhale-tfrecords-bb')\n",
        "GCS_DS_PATH=\"gs://kds-cb5f71b96dade328b9b531a32f301c705c11828ecc47258483b5e08f\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qy_v4volBnF",
        "outputId": "a2c146a8-c2e6-4b56-faf7-f97c5d996fb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "class config:\n",
        "    \n",
        "    SEED = 42\n",
        "    FOLD_TO_RUN = EXPERIMENT\n",
        "    FOLDS = 5\n",
        "    DEBUG = False\n",
        "    EVALUATE = True\n",
        "    RESUME = False\n",
        "    RESUME_EPOCH = None\n",
        "    \n",
        "    \n",
        "    ### Dataset\n",
        "    BATCH_SIZE = 4 * strategy.num_replicas_in_sync\n",
        "    IMAGE_SIZE = 896\n",
        "    # SWIN_IMAGE_SIZE = 224\n",
        "    # IMAGE_SIZE = 600\n",
        "    N_CLASSES = 15587\n",
        "    \n",
        "    ### Model\n",
        "    # model_type = 'effnetv1'\n",
        "    model_type = \"swin\"\n",
        "    model_type = \"hybrid\"\n",
        "\n",
        "    ## effnetv1 config\n",
        "    EFF_NET = 3\n",
        "    EFF_LAST_LAYER = \"block2c_project_conv\"\n",
        "    # effnetv2 config\n",
        "    # EFF_NETV2 = 's-21k-ft1k'\n",
        "\n",
        "    ## swin config\n",
        "    SWIN_NET = \"swin_base_224\"\n",
        "    # swin_tiny_224\n",
        "    # swin_small_224\n",
        "    # swin_base_224\n",
        "    # swin_base_384\n",
        "    # swin_large_224\n",
        "    # swin_large_384\n",
        "\n",
        "    ## Hybrid config\n",
        "    HYBRID_NET = \"hybrid_swin224_b5\"\n",
        "    \n",
        "\n",
        "\n",
        "    FREEZE_BATCH_NORM = False\n",
        "    head = 'arcface' \n",
        "    EPOCHS = 20\n",
        "    # EPOCHS = 1\n",
        "    LR = 0.001\n",
        "    message='baseline'\n",
        "    \n",
        "    ### Augmentations\n",
        "    CUTOUT = False\n",
        "    \n",
        "    ### Save-Directory\n",
        "    save_dir = save_dir\n",
        "    \n",
        "    ### Inference\n",
        "    KNN = 100\n",
        "    \n",
        "def count_data_items(filenames):\n",
        "    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) \n",
        "         for filename in filenames]\n",
        "    return np.sum(n)\n",
        "\n",
        "# Function to seed everything\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    \n",
        "def is_interactive():\n",
        "    return 'runtime'    in get_ipython().config.IPKernelApp.connection_file\n",
        "IS_INTERACTIVE = is_interactive()\n",
        "print(IS_INTERACTIVE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmJ-iPxVlBnG",
        "outputId": "2884777e-bae5-4cad-9f31-a750a0d7098b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hybrid_swin_base_224\n"
          ]
        }
      ],
      "source": [
        "MODEL_NAME = None\n",
        "if config.model_type == 'effnetv1':\n",
        "    MODEL_NAME = f'effnetv1_b{config.EFF_NET}'\n",
        "elif config.model_type == 'effnetv2':\n",
        "    MODEL_NAME = f'effnetv2_{config.EFF_NETV2}'\n",
        "elif config.model_type == \"swin\":\n",
        "    MODEL_NAME = f\"swin_{config.SWIN_NET}\"\n",
        "elif config.model_type == \"hybrid\":\n",
        "    MODEL_NAME = f\"hybrid_{config.SWIN_NET}\"\n",
        "\n",
        "config.MODEL_NAME = MODEL_NAME\n",
        "print(MODEL_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cHSmRioulBnG"
      },
      "outputs": [],
      "source": [
        "with open(config.save_dir+'/config.json', 'w') as fp:\n",
        "    json.dump({x:dict(config.__dict__)[x] for x in dict(config.__dict__) if not x.startswith('_')}, fp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3mR5hsXlBnG",
        "outputId": "5f87d18c-e955-4207-d5a5-979f591b07fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gs://kds-cb5f71b96dade328b9b531a32f301c705c11828ecc47258483b5e08f\n",
            "10 10 51033 27956\n"
          ]
        }
      ],
      "source": [
        "train_files = np.sort(np.array(tf.io.gfile.glob(GCS_DS_PATH + '/happywhale-2022-train*.tfrec')))\n",
        "test_files = np.sort(np.array(tf.io.gfile.glob(GCS_DS_PATH + '/happywhale-2022-test*.tfrec')))\n",
        "print(GCS_DS_PATH)\n",
        "print(len(train_files),len(test_files),count_data_items(train_files),count_data_items(test_files))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdeRjnlmlBnG"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fkhHQVYLlBnH"
      },
      "outputs": [],
      "source": [
        "def arcface_format(posting_id, image, label_group, matches):\n",
        "    return posting_id, {'inp1': image, 'inp2': label_group}, label_group, matches\n",
        "\n",
        "def arcface_inference_format(posting_id, image, label_group, matches):\n",
        "    return image,posting_id\n",
        "\n",
        "def arcface_eval_format(posting_id, image, label_group, matches):\n",
        "    return image,label_group\n",
        "\n",
        "# Data augmentation function\n",
        "def data_augment(posting_id, image, label_group, matches):\n",
        "\n",
        "    ### CUTOUT\n",
        "    if tf.random.uniform([])>0.5 and config.CUTOUT:\n",
        "      N_CUTOUT = 6\n",
        "      for cutouts in range(N_CUTOUT):\n",
        "        if tf.random.uniform([])>0.5:\n",
        "           DIM = config.IMAGE_SIZE\n",
        "           CUTOUT_LENGTH = DIM//8\n",
        "           x1 = tf.cast( tf.random.uniform([],0,DIM-CUTOUT_LENGTH),tf.int32)\n",
        "           x2 = tf.cast( tf.random.uniform([],0,DIM-CUTOUT_LENGTH),tf.int32)\n",
        "           filter_ = tf.concat([tf.zeros((x1,CUTOUT_LENGTH)),tf.ones((CUTOUT_LENGTH,CUTOUT_LENGTH)),tf.zeros((DIM-x1-CUTOUT_LENGTH,CUTOUT_LENGTH))],axis=0)\n",
        "           filter_ = tf.concat([tf.zeros((DIM,x2)),filter_,tf.zeros((DIM,DIM-x2-CUTOUT_LENGTH))],axis=1)\n",
        "           cutout = tf.reshape(1-filter_,(DIM,DIM,1))\n",
        "           image = cutout*image\n",
        "\n",
        "    image = tf.image.random_flip_left_right(image)\n",
        "    # image = tf.image.random_flip_up_down(image)\n",
        "    image = tf.image.random_hue(image, 0.01)\n",
        "    image = tf.image.random_saturation(image, 0.70, 1.30)\n",
        "    image = tf.image.random_contrast(image, 0.80, 1.20)\n",
        "    image = tf.image.random_brightness(image, 0.10)\n",
        "    return posting_id, image, label_group, matches\n",
        "\n",
        "# Function to decode our images\n",
        "# Updated to include crops.\n",
        "def decode_image(image_data, box):\n",
        "    if box is not None and box[0] != -1:\n",
        "        left, top, right, bottom = box[0], box[1], box[2], box[3]\n",
        "        bbs = tf.convert_to_tensor([top, left, bottom - top, right - left])\n",
        "        image = tf.io.decode_and_crop_jpeg(image_data, bbs, channels=3)\n",
        "    else:\n",
        "        image = tf.image.decode_jpeg(image_data, channels = 3)\n",
        "\n",
        "    image = tf.image.resize(image, [config.IMAGE_SIZE,config.IMAGE_SIZE])\n",
        "    image = tf.cast(image, tf.float32) / 255.0\n",
        "    return image\n",
        "\n",
        "# This function parse our images and also get the target variable\n",
        "def read_labeled_tfrecord(example):\n",
        "    LABELED_TFREC_FORMAT = {\n",
        "        \"image_name\": tf.io.FixedLenFeature([], tf.string),\n",
        "        \"image\": tf.io.FixedLenFeature([], tf.string),\n",
        "        \"target\": tf.io.FixedLenFeature([], tf.int64),\n",
        "        'detic_box': tf.io.FixedLenFeature([4], tf.int64),\n",
        "         # 'yolov5_box': tf.io.FixedLenFeature([4], tf.int64),\n",
        "    }\n",
        "\n",
        "    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n",
        "    posting_id = example['image_name']\n",
        "    bb = tf.cast(example['detic_box'], tf.int32)\n",
        "    image = decode_image(example['image'], bb)\n",
        "#     label_group = tf.one_hot(tf.cast(example['label_group'], tf.int32), depth = N_CLASSES)\n",
        "    label_group = tf.cast(example['target'], tf.int32)\n",
        "#     matches = example['matches']\n",
        "    matches = 1\n",
        "    return posting_id, image, label_group, matches\n",
        "\n",
        "# This function loads TF Records and parse them into tensors\n",
        "def load_dataset(filenames, ordered = False):\n",
        "    \n",
        "    ignore_order = tf.data.Options()\n",
        "    if not ordered:\n",
        "        ignore_order.experimental_deterministic = False \n",
        "        \n",
        "    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads = AUTO)\n",
        "#     dataset = dataset.cache()\n",
        "    dataset = dataset.with_options(ignore_order)\n",
        "    dataset = dataset.map(read_labeled_tfrecord, num_parallel_calls = AUTO) \n",
        "    return dataset\n",
        "\n",
        "# This function is to get our training tensors\n",
        "def get_training_dataset(filenames, shuffle=2048):\n",
        "    dataset = load_dataset(filenames, ordered = False)\n",
        "    dataset = dataset.map(data_augment, num_parallel_calls = AUTO)\n",
        "    dataset = dataset.map(arcface_format, num_parallel_calls = AUTO)\n",
        "    dataset = dataset.map(lambda posting_id, image, label_group, matches: (image, label_group))\n",
        "    dataset = dataset.repeat()\n",
        "    if shuffle is not None:\n",
        "        dataset = dataset.shuffle(2048)\n",
        "    dataset = dataset.batch(config.BATCH_SIZE, drop_remainder=True)\n",
        "    dataset = dataset.prefetch(AUTO)\n",
        "    return dataset\n",
        "\n",
        "# This function is to get our training tensors\n",
        "def get_val_dataset(filenames):\n",
        "    dataset = load_dataset(filenames, ordered = True)\n",
        "    dataset = dataset.map(data_augment, num_parallel_calls = AUTO)\n",
        "    dataset = dataset.map(arcface_format, num_parallel_calls = AUTO)\n",
        "    dataset = dataset.map(lambda posting_id, image, label_group, matches: (image, label_group))\n",
        "    dataset = dataset.batch(config.BATCH_SIZE, drop_remainder=True)\n",
        "    dataset = dataset.prefetch(AUTO)\n",
        "    return dataset\n",
        "\n",
        "# This function is to get our training tensors\n",
        "def get_eval_dataset(filenames, get_targets = True):\n",
        "    dataset = load_dataset(filenames, ordered = True)\n",
        "    dataset = dataset.map(data_augment, num_parallel_calls = AUTO)\n",
        "    dataset = dataset.map(arcface_eval_format, num_parallel_calls = AUTO)\n",
        "    if not get_targets:\n",
        "        dataset = dataset.map(lambda image, target: image)\n",
        "    dataset = dataset.batch(config.BATCH_SIZE, drop_remainder=True)\n",
        "    dataset = dataset.prefetch(AUTO)\n",
        "    return dataset\n",
        "\n",
        "# This function is to get our training tensors\n",
        "def get_test_dataset(filenames, get_names = True):\n",
        "    dataset = load_dataset(filenames, ordered = True)\n",
        "    dataset = dataset.map(data_augment, num_parallel_calls = AUTO)\n",
        "    dataset = dataset.map(arcface_inference_format, num_parallel_calls = AUTO)\n",
        "    if not get_names:\n",
        "        dataset = dataset.map(lambda image, posting_id: image)\n",
        "    dataset = dataset.batch(config.BATCH_SIZE, drop_remainder=True)\n",
        "    dataset = dataset.prefetch(AUTO)\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bllBU1GllBnK",
        "outputId": "d0f24f7d-c945-4380-88c5-ccfccadc2fd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "51033\n",
            "27956\n"
          ]
        }
      ],
      "source": [
        "row = 10; col = 8;\n",
        "row = min(row,config.BATCH_SIZE//col)\n",
        "N_TRAIN = count_data_items(train_files)\n",
        "print(N_TRAIN)\n",
        "ds = get_training_dataset(train_files)\n",
        "\n",
        "# for (sample,label) in ds:\n",
        "#     img = sample['inp1']\n",
        "#     plt.figure(figsize=(25,int(25*row/col)))\n",
        "#     for j in range(row*col):\n",
        "#         plt.subplot(row,col,j+1)\n",
        "#         plt.title(label[j].numpy())\n",
        "#         plt.axis('off')\n",
        "#         plt.imshow(img[j,])\n",
        "#     plt.show()\n",
        "#     break\n",
        "# print(img.shape)\n",
        "\n",
        "row = 10; col = 8;\n",
        "row = min(row,config.BATCH_SIZE//col)\n",
        "N_TEST = count_data_items(test_files)\n",
        "print(N_TEST)\n",
        "ds = get_test_dataset(test_files)\n",
        "\n",
        "# for (img,label) in ds:\n",
        "#     plt.figure(figsize=(25,int(25*row/col)))\n",
        "#     for j in range(row*col):\n",
        "#         plt.subplot(row,col,j+1)\n",
        "#         plt.title(label[j].numpy())\n",
        "#         plt.axis('off')\n",
        "#         plt.imshow(img[j,])\n",
        "#     plt.show()\n",
        "#     break\n",
        "# print(img.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xwxT80JlBnL"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLvz37WYlBnL"
      },
      "outputs": [],
      "source": [
        "# Arcmarginproduct class keras layer\n",
        "class ArcMarginProduct(tf.keras.layers.Layer):\n",
        "    '''\n",
        "    Implements large margin arc distance.\n",
        "\n",
        "    Reference:\n",
        "        https://arxiv.org/pdf/1801.07698.pdf\n",
        "        https://github.com/lyakaap/Landmark2019-1st-and-3rd-Place-Solution/\n",
        "            blob/master/src/modeling/metric_learning.py\n",
        "    '''\n",
        "    def __init__(self, n_classes, s=30, m=0.50, easy_margin=False,\n",
        "                 ls_eps=0.0, **kwargs):\n",
        "\n",
        "        super(ArcMarginProduct, self).__init__(**kwargs)\n",
        "\n",
        "        self.n_classes = n_classes\n",
        "        self.s = s\n",
        "        self.m = m\n",
        "        self.ls_eps = ls_eps\n",
        "        self.easy_margin = easy_margin\n",
        "        self.cos_m = tf.math.cos(m)\n",
        "        self.sin_m = tf.math.sin(m)\n",
        "        self.th = tf.math.cos(math.pi - m)\n",
        "        self.mm = tf.math.sin(math.pi - m) * m\n",
        "\n",
        "    def get_config(self):\n",
        "\n",
        "        config = super().get_config().copy()\n",
        "        config.update({\n",
        "            'n_classes': self.n_classes,\n",
        "            's': self.s,\n",
        "            'm': self.m,\n",
        "            'ls_eps': self.ls_eps,\n",
        "            'easy_margin': self.easy_margin,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        super(ArcMarginProduct, self).build(input_shape[0])\n",
        "\n",
        "        self.W = self.add_weight(\n",
        "            name='W',\n",
        "            shape=(int(input_shape[0][-1]), self.n_classes),\n",
        "            initializer='glorot_uniform',\n",
        "            dtype='float32',\n",
        "            trainable=True,\n",
        "            regularizer=None)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        X, y = inputs\n",
        "        y = tf.cast(y, dtype=tf.int32)\n",
        "        cosine = tf.matmul(\n",
        "            tf.math.l2_normalize(X, axis=1),\n",
        "            tf.math.l2_normalize(self.W, axis=0)\n",
        "        )\n",
        "        sine = tf.math.sqrt(1.0 - tf.math.pow(cosine, 2))\n",
        "        phi = cosine * self.cos_m - sine * self.sin_m\n",
        "        if self.easy_margin:\n",
        "            phi = tf.where(cosine > 0, phi, cosine)\n",
        "        else:\n",
        "            phi = tf.where(cosine > self.th, phi, cosine - self.mm)\n",
        "        one_hot = tf.cast(\n",
        "            tf.one_hot(y, depth=self.n_classes),\n",
        "            dtype=cosine.dtype\n",
        "        )\n",
        "        if self.ls_eps > 0:\n",
        "            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.n_classes\n",
        "\n",
        "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n",
        "        output *= self.s\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uOt-9EfvlBnL"
      },
      "outputs": [],
      "source": [
        "\n",
        "EFNS = [efn.EfficientNetB0, efn.EfficientNetB1, efn.EfficientNetB2, efn.EfficientNetB3, \n",
        "        efn.EfficientNetB4, efn.EfficientNetB5, efn.EfficientNetB6, efn.EfficientNetB7]\n",
        "\n",
        "def freeze_BN(model):\n",
        "    # Unfreeze layers while leaving BatchNorm layers frozen\n",
        "    for layer in model.layers:\n",
        "        if not isinstance(layer, tf.keras.layers.BatchNormalization):\n",
        "            layer.trainable = True\n",
        "        else:\n",
        "            layer.trainable = False\n",
        "\n",
        "# Function to create our EfficientNetB3 model\n",
        "def get_model():\n",
        "\n",
        "    if config.head=='arcface':\n",
        "        head = ArcMarginProduct\n",
        "    else:\n",
        "        assert 1==2, \"INVALID HEAD\"\n",
        "    \n",
        "    with strategy.scope():\n",
        "        margin = head(\n",
        "            n_classes = config.N_CLASSES, \n",
        "            s = 30,\n",
        "            m = 0.3,\n",
        "            name=f'head/{config.head}', \n",
        "            dtype='float32'\n",
        "            )\n",
        "\n",
        "        # inp = tf.keras.layers.Input(shape = [config.IMAGE_SIZE, config.IMAGE_SIZE, 3], name = 'inp1')\n",
        "        label = tf.keras.layers.Input(shape = (), name = 'inp2')\n",
        "        \n",
        "        if config.model_type == 'effnetv1':\n",
        "            x = EFNS[config.EFF_NET](weights = 'noisy-student', include_top = False)(inp)\n",
        "            embed = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "        elif config.model_type == 'effnetv2':\n",
        "            FEATURE_VECTOR = f'{EFFNETV2_ROOT}/tfhub_models/efficientnetv2-{config.EFF_NETV2}/feature_vector'\n",
        "            embed = tfhub.KerasLayer(FEATURE_VECTOR, trainable=True)(inp)\n",
        "        elif config.model_type == \"swin\":\n",
        "            x = SwinTransformer(config.SWIN_NET, include_top=False, pretrained=True, use_tpu=True)(inp)\n",
        "            embed = x\n",
        "            # embed = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "        \n",
        "        elif config.model_type == \"hybrid\":\n",
        "            \n",
        "            eff_obj = EFNS[config.EFF_NET](weights = 'noisy-student', include_top = False, input_shape = [config.IMAGE_SIZE, config.IMAGE_SIZE, 3])\n",
        "            eff_obj.layers[0]._name = 'inp1'\n",
        "            \n",
        "            x = tf.keras.layers.Conv2D(8, 1, 1)(eff_obj.get_layer(config.EFF_LAST_LAYER).output)\n",
        "            x = tf.keras.layers.Flatten()(x)\n",
        "            # model = tf.keras.models.Model(inputs = [eff_obj.input], outputs = [x])\n",
        "            # model.summary()\n",
        "            # x = tf.keras.layers.Dense(56*56*128)(x)\n",
        "            \n",
        "            x = tf.keras.layers.Reshape((-1, 128))(x)\n",
        "\n",
        "            swin_obj = tfimm.create_model(\"swin_base_patch4_window7_224\", pretrained=True)\n",
        "            # swin_obj = SwinTransformer(config.SWIN_NET, include_top=False, pretrained=True, use_tpu=True)\n",
        "            # x = swin_obj.output\n",
        "            for i, layer in enumerate(swin_obj.layers[1:]):\n",
        "                # print(i)\n",
        "                # print(tf.keras.models.Model(inputs = inp, outputs = x).outputs)\n",
        "                x = layer(x)\n",
        "            x = tf.keras.layers.Dense(1028, activation=\"linear\")(x)\n",
        "            x = tf.keras.layers.BatchNormalization()(x)\n",
        "            embed = tf.keras.layers.PReLU()(x)\n",
        "        # self.neck = nn.Sequential(\n",
        "        #         nn.Linear(backbone_out, self.embedding_size, bias=True),\n",
        "        #         nn.BatchNorm1d(self.embedding_size),\n",
        "        #         torch.nn.PReLU()\n",
        "        #     )\n",
        "\n",
        "        # embed = tf.keras.layers.Dropout(0.2)(embed)\n",
        "        # embed = tf.keras.layers.Dense(512)(embed)\n",
        "        x = margin([embed, label])\n",
        "        # x = embed\n",
        "        output = tf.keras.layers.Softmax(dtype='float32')(x)\n",
        "        \n",
        "        model = tf.keras.models.Model(inputs = [eff_obj.input, label], outputs = [output])\n",
        "        embed_model = tf.keras.models.Model(inputs = eff_obj.input, outputs = embed)  \n",
        "        \n",
        "        opt = tf.keras.optimizers.Adam(learning_rate = config.LR)\n",
        "        if config.FREEZE_BATCH_NORM:\n",
        "            freeze_BN(model)\n",
        "\n",
        "        model.compile(\n",
        "            optimizer = opt,\n",
        "            loss = [tf.keras.losses.SparseCategoricalCrossentropy()],\n",
        "            metrics = [tf.keras.metrics.SparseCategoricalAccuracy(),tf.keras.metrics.SparseTopKCategoricalAccuracy(k=5)]\n",
        "            ) \n",
        "        \n",
        "        return model,embed_model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TuS6dYEnlBnM"
      },
      "outputs": [],
      "source": [
        "class Snapshot(tf.keras.callbacks.Callback):\n",
        "    \n",
        "    def __init__(self,fold,snapshot_epochs=[]):\n",
        "        super(Snapshot, self).__init__()\n",
        "        self.snapshot_epochs = snapshot_epochs\n",
        "        self.fold = fold\n",
        "        \n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        # logs is a dictionary\n",
        "#         print(f\"epoch: {epoch}, train_acc: {logs['acc']}, valid_acc: {logs['val_acc']}\")\n",
        "        if epoch in self.snapshot_epochs: # your custom condition         \n",
        "            self.model.save_weights(config.save_dir+f\"/EF{config.MODEL_NAME}_epoch{epoch}.h5\")\n",
        "        self.model.save_weights(config.save_dir+f\"/{config.MODEL_NAME}_last.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "frdy2I9XlBnM",
        "outputId": "bb5bc58d-7b55-41e4-9aa7-6dedb31e12b5"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.LearningRateScheduler at 0x7fca1b0c1990>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "def get_lr_callback(plot=False):\n",
        "    lr_start   = 0.000001\n",
        "    lr_max     = 0.000005 * config.BATCH_SIZE  \n",
        "    lr_min     = 0.000001\n",
        "    lr_ramp_ep = 4\n",
        "    lr_sus_ep  = 0\n",
        "    lr_decay   = 0.9\n",
        "   \n",
        "    def lrfn(epoch):\n",
        "        if config.RESUME:\n",
        "            epoch = epoch + config.RESUME_EPOCH\n",
        "        if epoch < lr_ramp_ep:\n",
        "            lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start\n",
        "            \n",
        "        elif epoch < lr_ramp_ep + lr_sus_ep:\n",
        "            lr = lr_max\n",
        "            \n",
        "        else:\n",
        "            lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n",
        "            \n",
        "        return lr\n",
        "        \n",
        "    if plot:\n",
        "        epochs = list(range(config.EPOCHS))\n",
        "        learning_rates = [lrfn(x) for x in epochs]\n",
        "        plt.scatter(epochs,learning_rates)\n",
        "        plt.show()\n",
        "\n",
        "    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=False)\n",
        "    return lr_callback\n",
        "\n",
        "get_lr_callback(plot=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWbTjl_xlBnM"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SF3sgQWLlBnM",
        "outputId": "93859956-a6ce-4ff8-edc8-b43da7939148"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8 2 40827 10206\n"
          ]
        }
      ],
      "source": [
        "TRAINING_FILENAMES = [x for i,x in enumerate(train_files) if i%config.FOLDS!=config.FOLD_TO_RUN]\n",
        "VALIDATION_FILENAMES = [x for i,x in enumerate(train_files) if i%config.FOLDS==config.FOLD_TO_RUN]\n",
        "print(len(TRAINING_FILENAMES),len(VALIDATION_FILENAMES),count_data_items(TRAINING_FILENAMES),count_data_items(VALIDATION_FILENAMES))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6wE7GgFylBnN"
      },
      "outputs": [],
      "source": [
        "if config.DEBUG:\n",
        "    TRAINING_FILENAMES = [TRAINING_FILENAMES[0]]\n",
        "    VALIDATION_FILENAMES = [VALIDATION_FILENAMES[0]]\n",
        "    print(len(TRAINING_FILENAMES),len(VALIDATION_FILENAMES),count_data_items(TRAINING_FILENAMES),count_data_items(VALIDATION_FILENAMES))\n",
        "    test_files = [test_files[0]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6Nso-T3lBnN",
        "outputId": "4a76051d-7078-403f-a168-348e1aafcb8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "WARNING:tfimm.utils.timm:All PyTorch model weights were used when initializing SwinTransformer.\n",
            "WARNING:tfimm.utils.timm:All the weights of SwinTransformer were initialized from the PyTorch model.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " inp1 (InputLayer)              [(None, 896, 896, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " stem_conv (Conv2D)             (None, 448, 448, 40  1080        ['inp1[0][0]']                   \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " stem_bn (BatchNormalization)   (None, 448, 448, 40  160         ['stem_conv[0][0]']              \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " stem_activation (Activation)   (None, 448, 448, 40  0           ['stem_bn[0][0]']                \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " block1a_dwconv (DepthwiseConv2  (None, 448, 448, 40  360        ['stem_activation[0][0]']        \n",
            " D)                             )                                                                 \n",
            "                                                                                                  \n",
            " block1a_bn (BatchNormalization  (None, 448, 448, 40  160        ['block1a_dwconv[0][0]']         \n",
            " )                              )                                                                 \n",
            "                                                                                                  \n",
            " block1a_activation (Activation  (None, 448, 448, 40  0          ['block1a_bn[0][0]']             \n",
            " )                              )                                                                 \n",
            "                                                                                                  \n",
            " block1a_se_squeeze (GlobalAver  (None, 40)          0           ['block1a_activation[0][0]']     \n",
            " agePooling2D)                                                                                    \n",
            "                                                                                                  \n",
            " block1a_se_reshape (Reshape)   (None, 1, 1, 40)     0           ['block1a_se_squeeze[0][0]']     \n",
            "                                                                                                  \n",
            " block1a_se_reduce (Conv2D)     (None, 1, 1, 10)     410         ['block1a_se_reshape[0][0]']     \n",
            "                                                                                                  \n",
            " block1a_se_expand (Conv2D)     (None, 1, 1, 40)     440         ['block1a_se_reduce[0][0]']      \n",
            "                                                                                                  \n",
            " block1a_se_excite (Multiply)   (None, 448, 448, 40  0           ['block1a_activation[0][0]',     \n",
            "                                )                                 'block1a_se_expand[0][0]']      \n",
            "                                                                                                  \n",
            " block1a_project_conv (Conv2D)  (None, 448, 448, 24  960         ['block1a_se_excite[0][0]']      \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " block1a_project_bn (BatchNorma  (None, 448, 448, 24  96         ['block1a_project_conv[0][0]']   \n",
            " lization)                      )                                                                 \n",
            "                                                                                                  \n",
            " block1b_dwconv (DepthwiseConv2  (None, 448, 448, 24  216        ['block1a_project_bn[0][0]']     \n",
            " D)                             )                                                                 \n",
            "                                                                                                  \n",
            " block1b_bn (BatchNormalization  (None, 448, 448, 24  96         ['block1b_dwconv[0][0]']         \n",
            " )                              )                                                                 \n",
            "                                                                                                  \n",
            " block1b_activation (Activation  (None, 448, 448, 24  0          ['block1b_bn[0][0]']             \n",
            " )                              )                                                                 \n",
            "                                                                                                  \n",
            " block1b_se_squeeze (GlobalAver  (None, 24)          0           ['block1b_activation[0][0]']     \n",
            " agePooling2D)                                                                                    \n",
            "                                                                                                  \n",
            " block1b_se_reshape (Reshape)   (None, 1, 1, 24)     0           ['block1b_se_squeeze[0][0]']     \n",
            "                                                                                                  \n",
            " block1b_se_reduce (Conv2D)     (None, 1, 1, 6)      150         ['block1b_se_reshape[0][0]']     \n",
            "                                                                                                  \n",
            " block1b_se_expand (Conv2D)     (None, 1, 1, 24)     168         ['block1b_se_reduce[0][0]']      \n",
            "                                                                                                  \n",
            " block1b_se_excite (Multiply)   (None, 448, 448, 24  0           ['block1b_activation[0][0]',     \n",
            "                                )                                 'block1b_se_expand[0][0]']      \n",
            "                                                                                                  \n",
            " block1b_project_conv (Conv2D)  (None, 448, 448, 24  576         ['block1b_se_excite[0][0]']      \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " block1b_project_bn (BatchNorma  (None, 448, 448, 24  96         ['block1b_project_conv[0][0]']   \n",
            " lization)                      )                                                                 \n",
            "                                                                                                  \n",
            " block1b_drop (FixedDropout)    (None, 448, 448, 24  0           ['block1b_project_bn[0][0]']     \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " block1b_add (Add)              (None, 448, 448, 24  0           ['block1b_drop[0][0]',           \n",
            "                                )                                 'block1a_project_bn[0][0]']     \n",
            "                                                                                                  \n",
            " block2a_expand_conv (Conv2D)   (None, 448, 448, 14  3456        ['block1b_add[0][0]']            \n",
            "                                4)                                                                \n",
            "                                                                                                  \n",
            " block2a_expand_bn (BatchNormal  (None, 448, 448, 14  576        ['block2a_expand_conv[0][0]']    \n",
            " ization)                       4)                                                                \n",
            "                                                                                                  \n",
            " block2a_expand_activation (Act  (None, 448, 448, 14  0          ['block2a_expand_bn[0][0]']      \n",
            " ivation)                       4)                                                                \n",
            "                                                                                                  \n",
            " block2a_dwconv (DepthwiseConv2  (None, 224, 224, 14  1296       ['block2a_expand_activation[0][0]\n",
            " D)                             4)                               ']                               \n",
            "                                                                                                  \n",
            " block2a_bn (BatchNormalization  (None, 224, 224, 14  576        ['block2a_dwconv[0][0]']         \n",
            " )                              4)                                                                \n",
            "                                                                                                  \n",
            " block2a_activation (Activation  (None, 224, 224, 14  0          ['block2a_bn[0][0]']             \n",
            " )                              4)                                                                \n",
            "                                                                                                  \n",
            " block2a_se_squeeze (GlobalAver  (None, 144)         0           ['block2a_activation[0][0]']     \n",
            " agePooling2D)                                                                                    \n",
            "                                                                                                  \n",
            " block2a_se_reshape (Reshape)   (None, 1, 1, 144)    0           ['block2a_se_squeeze[0][0]']     \n",
            "                                                                                                  \n",
            " block2a_se_reduce (Conv2D)     (None, 1, 1, 6)      870         ['block2a_se_reshape[0][0]']     \n",
            "                                                                                                  \n",
            " block2a_se_expand (Conv2D)     (None, 1, 1, 144)    1008        ['block2a_se_reduce[0][0]']      \n",
            "                                                                                                  \n",
            " block2a_se_excite (Multiply)   (None, 224, 224, 14  0           ['block2a_activation[0][0]',     \n",
            "                                4)                                'block2a_se_expand[0][0]']      \n",
            "                                                                                                  \n",
            " block2a_project_conv (Conv2D)  (None, 224, 224, 32  4608        ['block2a_se_excite[0][0]']      \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " block2a_project_bn (BatchNorma  (None, 224, 224, 32  128        ['block2a_project_conv[0][0]']   \n",
            " lization)                      )                                                                 \n",
            "                                                                                                  \n",
            " block2b_expand_conv (Conv2D)   (None, 224, 224, 19  6144        ['block2a_project_bn[0][0]']     \n",
            "                                2)                                                                \n",
            "                                                                                                  \n",
            " block2b_expand_bn (BatchNormal  (None, 224, 224, 19  768        ['block2b_expand_conv[0][0]']    \n",
            " ization)                       2)                                                                \n",
            "                                                                                                  \n",
            " block2b_expand_activation (Act  (None, 224, 224, 19  0          ['block2b_expand_bn[0][0]']      \n",
            " ivation)                       2)                                                                \n",
            "                                                                                                  \n",
            " block2b_dwconv (DepthwiseConv2  (None, 224, 224, 19  1728       ['block2b_expand_activation[0][0]\n",
            " D)                             2)                               ']                               \n",
            "                                                                                                  \n",
            " block2b_bn (BatchNormalization  (None, 224, 224, 19  768        ['block2b_dwconv[0][0]']         \n",
            " )                              2)                                                                \n",
            "                                                                                                  \n",
            " block2b_activation (Activation  (None, 224, 224, 19  0          ['block2b_bn[0][0]']             \n",
            " )                              2)                                                                \n",
            "                                                                                                  \n",
            " block2b_se_squeeze (GlobalAver  (None, 192)         0           ['block2b_activation[0][0]']     \n",
            " agePooling2D)                                                                                    \n",
            "                                                                                                  \n",
            " block2b_se_reshape (Reshape)   (None, 1, 1, 192)    0           ['block2b_se_squeeze[0][0]']     \n",
            "                                                                                                  \n",
            " block2b_se_reduce (Conv2D)     (None, 1, 1, 8)      1544        ['block2b_se_reshape[0][0]']     \n",
            "                                                                                                  \n",
            " block2b_se_expand (Conv2D)     (None, 1, 1, 192)    1728        ['block2b_se_reduce[0][0]']      \n",
            "                                                                                                  \n",
            " block2b_se_excite (Multiply)   (None, 224, 224, 19  0           ['block2b_activation[0][0]',     \n",
            "                                2)                                'block2b_se_expand[0][0]']      \n",
            "                                                                                                  \n",
            " block2b_project_conv (Conv2D)  (None, 224, 224, 32  6144        ['block2b_se_excite[0][0]']      \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " block2b_project_bn (BatchNorma  (None, 224, 224, 32  128        ['block2b_project_conv[0][0]']   \n",
            " lization)                      )                                                                 \n",
            "                                                                                                  \n",
            " block2b_drop (FixedDropout)    (None, 224, 224, 32  0           ['block2b_project_bn[0][0]']     \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " block2b_add (Add)              (None, 224, 224, 32  0           ['block2b_drop[0][0]',           \n",
            "                                )                                 'block2a_project_bn[0][0]']     \n",
            "                                                                                                  \n",
            " block2c_expand_conv (Conv2D)   (None, 224, 224, 19  6144        ['block2b_add[0][0]']            \n",
            "                                2)                                                                \n",
            "                                                                                                  \n",
            " block2c_expand_bn (BatchNormal  (None, 224, 224, 19  768        ['block2c_expand_conv[0][0]']    \n",
            " ization)                       2)                                                                \n",
            "                                                                                                  \n",
            " block2c_expand_activation (Act  (None, 224, 224, 19  0          ['block2c_expand_bn[0][0]']      \n",
            " ivation)                       2)                                                                \n",
            "                                                                                                  \n",
            " block2c_dwconv (DepthwiseConv2  (None, 224, 224, 19  1728       ['block2c_expand_activation[0][0]\n",
            " D)                             2)                               ']                               \n",
            "                                                                                                  \n",
            " block2c_bn (BatchNormalization  (None, 224, 224, 19  768        ['block2c_dwconv[0][0]']         \n",
            " )                              2)                                                                \n",
            "                                                                                                  \n",
            " block2c_activation (Activation  (None, 224, 224, 19  0          ['block2c_bn[0][0]']             \n",
            " )                              2)                                                                \n",
            "                                                                                                  \n",
            " block2c_se_squeeze (GlobalAver  (None, 192)         0           ['block2c_activation[0][0]']     \n",
            " agePooling2D)                                                                                    \n",
            "                                                                                                  \n",
            " block2c_se_reshape (Reshape)   (None, 1, 1, 192)    0           ['block2c_se_squeeze[0][0]']     \n",
            "                                                                                                  \n",
            " block2c_se_reduce (Conv2D)     (None, 1, 1, 8)      1544        ['block2c_se_reshape[0][0]']     \n",
            "                                                                                                  \n",
            " block2c_se_expand (Conv2D)     (None, 1, 1, 192)    1728        ['block2c_se_reduce[0][0]']      \n",
            "                                                                                                  \n",
            " block2c_se_excite (Multiply)   (None, 224, 224, 19  0           ['block2c_activation[0][0]',     \n",
            "                                2)                                'block2c_se_expand[0][0]']      \n",
            "                                                                                                  \n",
            " block2c_project_conv (Conv2D)  (None, 224, 224, 32  6144        ['block2c_se_excite[0][0]']      \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, 224, 224, 8)  264         ['block2c_project_conv[0][0]']   \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 401408)       0           ['conv2d[0][0]']                 \n",
            "                                                                                                  \n",
            " reshape (Reshape)              (None, 3136, 128)    0           ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 3136, 128)    0           ['reshape[0][0]']                \n",
            "                                                                                                  \n",
            " layers/0 (SwinTransformerStage  (None, 784, 256)    688459      ['dropout[0][0]']                \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " layers/1 (SwinTransformerStage  (None, 196, 512)    2151779     ['layers/0[0][0]']               \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " layers/2 (SwinTransformerStage  (None, 49, 1024)    59022495    ['layers/1[0][0]']               \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " layers/3 (SwinTransformerStage  (None, 49, 1024)    25208068    ['layers/2[0][0]']               \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " norm (LayerNormalization)      (None, 49, 1024)     2048        ['layers/3[0][0]']               \n",
            "                                                                                                  \n",
            " global_average_pooling1d (Glob  (None, 1024)        0           ['norm[0][0]']                   \n",
            " alAveragePooling1D)                                                                              \n",
            "                                                                                                  \n",
            " head (Dense)                   (None, 1000)         1025000     ['global_average_pooling1d[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 1028)         1029028     ['head[0][0]']                   \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 1028)        4112        ['dense[0][0]']                  \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " p_re_lu (PReLU)                (None, 1028)         1028        ['batch_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " inp2 (InputLayer)              [(None,)]            0           []                               \n",
            "                                                                                                  \n",
            " head/arcface (ArcMarginProduct  (None, 15587)       16023436    ['p_re_lu[0][0]',                \n",
            " )                                                                'inp2[0][0]']                   \n",
            "                                                                                                  \n",
            " softmax (Softmax)              (None, 15587)        0           ['head/arcface[0][0]']           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 105,210,979\n",
            "Trainable params: 104,870,226\n",
            "Non-trainable params: 340,753\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "seed_everything(config.SEED)\n",
        "VERBOSE = 1\n",
        "train_dataset = get_training_dataset(TRAINING_FILENAMES)\n",
        "val_dataset = get_val_dataset(VALIDATION_FILENAMES)\n",
        "STEPS_PER_EPOCH = count_data_items(TRAINING_FILENAMES) // config.BATCH_SIZE\n",
        "# TEST_STEPS = -(-count_data_items(test_files) // BATCH_SIZE)             # The \"-(-//)\" trick rounds up instead of down :-)\n",
        "\n",
        "train_logger = tf.keras.callbacks.CSVLogger(config.save_dir+'/training-log-fold-%i.h5.csv'%config.FOLD_TO_RUN)\n",
        "# SAVE BEST MODEL EACH FOLD        \n",
        "sv_loss = tf.keras.callbacks.ModelCheckpoint(\n",
        "    config.save_dir+f\"/{config.MODEL_NAME}_loss.h5\", monitor='val_loss', verbose=0, save_best_only=True,\n",
        "    save_weights_only=True, mode='min', save_freq='epoch')\n",
        "# BUILD MODEL\n",
        "K.clear_session()\n",
        "model,embed_model = get_model()\n",
        "snap = Snapshot(fold=config.FOLD_TO_RUN,snapshot_epochs=[5,8])\n",
        "model.summary()\n",
        "\n",
        "if config.RESUME:\n",
        "    model.load_weights(config.resume_model_wts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZxgDCIHlBnN",
        "outputId": "60e04d23-c8a3-4358-9641-a69776823bb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#### Image Size 896 with hybrid_swin_base_224 and batch_size 32\n",
            "Epoch 1/20\n",
            "1275/1275 [==============================] - 982s 656ms/step - loss: 18.6444 - sparse_categorical_accuracy: 0.0000e+00 - sparse_top_k_categorical_accuracy: 0.0000e+00 - val_loss: 18.5612 - val_sparse_categorical_accuracy: 0.0000e+00 - val_sparse_top_k_categorical_accuracy: 0.0000e+00 - lr: 1.0000e-06\n",
            "Epoch 2/20\n",
            "1275/1275 [==============================] - 559s 438ms/step - loss: 17.6404 - sparse_categorical_accuracy: 0.0000e+00 - sparse_top_k_categorical_accuracy: 0.0000e+00 - val_loss: 18.1723 - val_sparse_categorical_accuracy: 0.0000e+00 - val_sparse_top_k_categorical_accuracy: 0.0000e+00 - lr: 4.0750e-05\n",
            "Epoch 3/20\n",
            "1275/1275 [==============================] - 555s 436ms/step - loss: 17.1618 - sparse_categorical_accuracy: 0.0000e+00 - sparse_top_k_categorical_accuracy: 0.0000e+00 - val_loss: 18.0079 - val_sparse_categorical_accuracy: 0.0000e+00 - val_sparse_top_k_categorical_accuracy: 0.0000e+00 - lr: 8.0500e-05\n",
            "Epoch 4/20\n",
            "1275/1275 [==============================] - 553s 434ms/step - loss: 16.5452 - sparse_categorical_accuracy: 0.0000e+00 - sparse_top_k_categorical_accuracy: 0.0000e+00 - val_loss: 17.3799 - val_sparse_categorical_accuracy: 0.0000e+00 - val_sparse_top_k_categorical_accuracy: 0.0000e+00 - lr: 1.2025e-04\n",
            "Epoch 5/20\n",
            "1275/1275 [==============================] - 562s 440ms/step - loss: 15.8376 - sparse_categorical_accuracy: 0.0000e+00 - sparse_top_k_categorical_accuracy: 0.0000e+00 - val_loss: 16.8788 - val_sparse_categorical_accuracy: 0.0000e+00 - val_sparse_top_k_categorical_accuracy: 0.0000e+00 - lr: 1.6000e-04\n",
            "Epoch 6/20\n",
            "1275/1275 [==============================] - 557s 437ms/step - loss: 14.9524 - sparse_categorical_accuracy: 0.0000e+00 - sparse_top_k_categorical_accuracy: 0.0000e+00 - val_loss: 15.5290 - val_sparse_categorical_accuracy: 0.0000e+00 - val_sparse_top_k_categorical_accuracy: 0.0000e+00 - lr: 1.4410e-04\n",
            "Epoch 7/20\n",
            "1275/1275 [==============================] - 550s 431ms/step - loss: nan - sparse_categorical_accuracy: 2.4510e-05 - sparse_top_k_categorical_accuracy: 0.0000e+00 - val_loss: nan - val_sparse_categorical_accuracy: 0.0000e+00 - val_sparse_top_k_categorical_accuracy: 0.0000e+00 - lr: 1.2979e-04\n",
            "Epoch 8/20\n",
            " 645/1275 [==============>...............] - ETA: 4:02 - loss: nan - sparse_categorical_accuracy: 0.0000e+00 - sparse_top_k_categorical_accuracy: 0.0000e+00"
          ]
        }
      ],
      "source": [
        "print(f'#### Image Size {config.IMAGE_SIZE} with {config.MODEL_NAME} and batch_size {config.BATCH_SIZE}')\n",
        "\n",
        "history = model.fit(train_dataset,\n",
        "                validation_data = val_dataset,\n",
        "                steps_per_epoch = STEPS_PER_EPOCH,\n",
        "                epochs = config.EPOCHS,\n",
        "                callbacks = [get_lr_callback(),train_logger,sv_loss],\n",
        "                verbose = VERBOSE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4W7VhQNunMH0"
      },
      "outputs": [],
      "source": [
        "print('Learning rate:', history.history['lr'])\n",
        "n_col, n_row = 3, 1\n",
        "fig, axes = plt.subplots(n_row, n_col, figsize=(9*n_col, 5*n_row))\n",
        "for idx, (ax, name) in enumerate(zip(axes, ['loss', 'sparse_categorical_accuracy', 'sparse_top_k_categorical_accuracy'])):\n",
        "    ax.title.set_text(name)\n",
        "    ax.plot(history.history[name], label=name)\n",
        "    ax.plot(history.history['val_'+name], label='val_'+name)\n",
        "    ax.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ZkHO0IslBnN"
      },
      "outputs": [],
      "source": [
        "model.load_weights(config.save_dir+f\"/{config.MODEL_NAME}_loss.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3lyhO_PlBnN"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DXIJjLKElBnN"
      },
      "outputs": [],
      "source": [
        "def get_ids(filename):\n",
        "    ds = get_test_dataset([filename],get_names=True).map(lambda image, image_name: image_name).unbatch()\n",
        "    NUM_IMAGES = count_data_items([filename])\n",
        "    ids = next(iter(ds.batch(NUM_IMAGES))).numpy().astype('U')\n",
        "    return ids\n",
        "\n",
        "def get_targets(filename):\n",
        "    ds = get_eval_dataset([filename],get_targets=True).map(lambda image, target: target).unbatch()\n",
        "    NUM_IMAGES = count_data_items([filename])\n",
        "    ids = next(iter(ds.batch(NUM_IMAGES))).numpy()\n",
        "    return ids\n",
        "\n",
        "def get_embeddings(filename, steps=None):\n",
        "    ds = get_test_dataset([filename],get_names=False)\n",
        "    embeddings = embed_model.predict(ds, steps=steps, verbose=0)\n",
        "    return embeddings\n",
        "\n",
        "def get_predictions(test_df,threshold=0.2):\n",
        "    predictions = {}\n",
        "    for i,row in tqdm(test_df.iterrows()):\n",
        "        if row.image in predictions:\n",
        "            if len(predictions[row.image])==5:\n",
        "                continue\n",
        "            predictions[row.image].append(row.target)\n",
        "        elif row.confidence>threshold:\n",
        "            predictions[row.image] = [row.target,'new_individual']\n",
        "        else:\n",
        "            predictions[row.image] = ['new_individual',row.target]\n",
        "\n",
        "    for x in tqdm(predictions):\n",
        "        if len(predictions[x])<5:\n",
        "            remaining = [y for y in sample_list if y not in predictions]\n",
        "            predictions[x] = predictions[x]+remaining\n",
        "            predictions[x] = predictions[x][:5]\n",
        "        \n",
        "    return predictions\n",
        "\n",
        "def map_per_image(label, predictions):\n",
        "    \"\"\"Computes the precision score of one image.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    label : string\n",
        "            The true label of the image\n",
        "    predictions : list\n",
        "            A list of predicted elements (order does matter, 5 predictions allowed per image)\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    score : double\n",
        "    \"\"\"\n",
        "    try:\n",
        "        return 1 / (predictions[:5].index(label) + 1)\n",
        "    except ValueError:\n",
        "        return 0.0\n",
        "    \n",
        "f = open ('/content/individual_ids.json', \"r\")\n",
        "target_encodings = json.loads(f.read())\n",
        "target_encodings = {target_encodings[x]:x for x in target_encodings}\n",
        "sample_list = ['938b7e931166', '5bf17305f073', '7593d2aee842', '7362d7a01d00','956562ff2888']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TRAINING_FILENAMES"
      ],
      "metadata": {
        "id": "OWPES_dnZG9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tv4W6CCtlBnO"
      },
      "outputs": [],
      "source": [
        "train_targets = []\n",
        "train_embeddings = []\n",
        "for filename in tqdm(TRAINING_FILENAMES):\n",
        "    fold_idx = int(re.search(\"train-([0-9])\", filename)[1])\n",
        "    fold_actual_size = 5104 if fold_idx <= 2 else 5103\n",
        "    embeddings = get_embeddings(filename)[:fold_actual_size]\n",
        "    targets = get_targets(filename)[:fold_actual_size]\n",
        "    train_embeddings.append(embeddings)\n",
        "    train_targets.append(targets)\n",
        "train_embeddings = np.concatenate(train_embeddings)\n",
        "train_targets = np.concatenate(train_targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0yFHoAM1lBnO"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import NearestNeighbors\n",
        "neigh = NearestNeighbors(n_neighbors=config.KNN,metric='cosine')\n",
        "neigh.fit(train_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test_ids = []\n",
        "# test_nn_distances = []\n",
        "# test_nn_idxs = []\n",
        "# val_targets = []\n",
        "# val_embeddings = []\n",
        "# for filename in tqdm(VALIDATION_FILENAMES):\n",
        "#     embeddings = get_embeddings(filename)\n",
        "#     targets = get_targets(filename)\n",
        "#     ids = get_ids(filename)\n",
        "#     distances,idxs = neigh.kneighbors(embeddings, config.KNN, return_distance=True)\n",
        "#     test_ids.append(ids)\n",
        "#     test_nn_idxs.append(idxs)\n",
        "#     test_nn_distances.append(distances)\n",
        "#     val_embeddings.append(embeddings)\n",
        "#     val_targets.append(targets)\n",
        "# test_nn_distances = np.concatenate(test_nn_distances)\n",
        "# test_nn_idxs = np.concatenate(test_nn_idxs)\n",
        "# test_ids = np.concatenate(test_ids)\n",
        "# val_embeddings = np.concatenate(val_embeddings)\n",
        "# val_targets = np.concatenate(val_targets)\n",
        "\n",
        "val_ids = []\n",
        "val_nn_distances = []\n",
        "val_nn_idxs = []\n",
        "val_targets = []\n",
        "val_embeddings = []\n",
        "for filename in tqdm(VALIDATION_FILENAMES):\n",
        "    fold_idx = int(re.search(\"train-([0-9])\", filename)[1])\n",
        "    fold_actual_size = 5104 if fold_idx <= 2 else 5103\n",
        "    embeddings = get_embeddings(filename)[:fold_actual_size]\n",
        "    targets = get_targets(filename)[:fold_actual_size]\n",
        "    ids = get_ids(filename)[:fold_actual_size]\n",
        "    distances,idxs = neigh.kneighbors(embeddings, config.KNN, return_distance=True)\n",
        "    val_ids.append(ids)\n",
        "    val_nn_idxs.append(idxs)\n",
        "    val_nn_distances.append(distances)\n",
        "    val_embeddings.append(embeddings)\n",
        "    val_targets.append(targets)\n",
        "val_nn_distances = np.concatenate(val_nn_distances)\n",
        "val_nn_idxs = np.concatenate(val_nn_idxs)\n",
        "val_ids = np.concatenate(val_ids)\n",
        "val_embeddings = np.concatenate(val_embeddings)\n",
        "val_targets = np.concatenate(val_targets)"
      ],
      "metadata": {
        "id": "tx6MsXe0rXEP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h7cQmihYlBnO"
      },
      "outputs": [],
      "source": [
        "# allowed_targets = set([target_encodings[x] for x in np.unique(train_targets)])\n",
        "# val_targets_df = pd.DataFrame(np.stack([test_ids,val_targets],axis=1),columns=['image','target'])\n",
        "# val_targets_df['target'] = val_targets_df['target'].astype(int).map(target_encodings)\n",
        "# val_targets_df.loc[~val_targets_df.target.isin(allowed_targets),'target'] = 'new_individual'\n",
        "# val_targets_df.target.value_counts()\n",
        "\n",
        "allowed_targets = set([target_encodings[x] for x in np.unique(train_targets)])\n",
        "val_targets_df = pd.DataFrame(np.stack([val_ids,val_targets],axis=1),columns=['image','target'])\n",
        "val_targets_df['target'] = val_targets_df['target'].astype(int).map(target_encodings)\n",
        "val_targets_df.loc[~val_targets_df.target.isin(allowed_targets),'target'] = 'new_individual'\n",
        "val_targets_df.target.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3lFAet12lBnO"
      },
      "outputs": [],
      "source": [
        "# test_df = []\n",
        "# for i in tqdm(range(len(test_ids))):\n",
        "#     id_ = test_ids[i]\n",
        "#     targets = train_targets[test_nn_idxs[i]]\n",
        "#     distances = test_nn_distances[i]\n",
        "#     subset_preds = pd.DataFrame(np.stack([targets,distances],axis=1),columns=['target','distances'])\n",
        "#     subset_preds['image'] = id_\n",
        "#     test_df.append(subset_preds)\n",
        "# test_df = pd.concat(test_df).reset_index(drop=True)\n",
        "# test_df['confidence'] = 1-test_df['distances']\n",
        "# test_df = test_df.groupby(['image','target']).confidence.max().reset_index()\n",
        "# test_df = test_df.sort_values('confidence',ascending=False).reset_index(drop=True)\n",
        "# test_df['target'] = test_df['target'].map(target_encodings)\n",
        "# test_df.to_csv('val_neighbors.csv')\n",
        "# test_df.image.value_counts().value_counts()\n",
        "\n",
        "val_df = []\n",
        "for i in tqdm(range(len(val_ids))):\n",
        "    id_ = val_ids[i]\n",
        "    targets = train_targets[val_nn_idxs[i]]\n",
        "    distances = val_nn_distances[i]\n",
        "    subset_preds = pd.DataFrame(np.stack([targets,distances],axis=1),columns=['target','distances'])\n",
        "    subset_preds['image'] = id_\n",
        "    val_df.append(subset_preds)\n",
        "val_df = pd.concat(val_df).reset_index(drop=True)\n",
        "val_df['confidence'] = 1-val_df['distances']\n",
        "val_df = val_df.groupby(['image','target']).confidence.max().reset_index()\n",
        "val_df = val_df.sort_values('confidence',ascending=False).reset_index(drop=True)\n",
        "val_df['target'] = val_df['target'].map(target_encodings)\n",
        "val_df.to_csv('val_neighbors.csv')\n",
        "val_df.image.value_counts().value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N1uO9aFZlBnP"
      },
      "outputs": [],
      "source": [
        "# ## Compute CV\n",
        "# best_th = 0\n",
        "# best_cv = 0\n",
        "# for th in [0.1*x for x in range(11)]:\n",
        "#     all_preds = get_predictions(test_df,threshold=th)\n",
        "#     cv = 0\n",
        "#     for i,row in val_targets_df.iterrows():\n",
        "#         target = row.target\n",
        "#         preds = all_preds[row.image]\n",
        "#         val_targets_df.loc[i,th] = map_per_image(target,preds)\n",
        "#     cv = val_targets_df[th].mean()\n",
        "#     print(f\"CV at threshold {th}: {cv}\")\n",
        "#     if cv>best_cv:\n",
        "#         best_th = th\n",
        "#         best_cv = cv\n",
        "\n",
        "## Compute CV\n",
        "best_th = 0\n",
        "best_cv = 0\n",
        "for th in [0.1*x for x in range(11)]:\n",
        "    all_preds = get_predictions(val_df,threshold=th)\n",
        "    cv = 0\n",
        "    for i,row in val_targets_df.iterrows():\n",
        "        target = row.target\n",
        "        preds = all_preds[row.image]\n",
        "        val_targets_df.loc[i,th] = map_per_image(target,preds)\n",
        "    cv = val_targets_df[th].mean()\n",
        "    print(f\"CV at threshold {th}: {cv}\")\n",
        "    if cv>best_cv:\n",
        "        best_th = th\n",
        "        best_cv = cv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yaZSpRjWlBnP"
      },
      "outputs": [],
      "source": [
        "print(\"Best threshold\",best_th)\n",
        "print(\"Best cv\",best_cv)\n",
        "val_targets_df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i1PlfufdlBnP"
      },
      "outputs": [],
      "source": [
        "## Adjustment: Since Public lb has nearly 10% 'new_individual' (Be Careful for private LB)\n",
        "val_targets_df['is_new_individual'] = val_targets_df.target=='new_individual'\n",
        "print(val_targets_df.is_new_individual.value_counts().to_dict())\n",
        "val_scores = val_targets_df.groupby('is_new_individual').mean().T\n",
        "val_scores['adjusted_cv'] = val_scores[True]*0.1+val_scores[False]*0.9\n",
        "best_threshold_adjusted = val_scores['adjusted_cv'].idxmax()\n",
        "print(\"best_threshold\",best_threshold_adjusted)\n",
        "val_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aGN1hi8lBnP"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8GNCYQdtlBnP"
      },
      "outputs": [],
      "source": [
        "train_embeddings = np.concatenate([train_embeddings,val_embeddings])\n",
        "train_targets = np.concatenate([train_targets,val_targets])\n",
        "print(train_embeddings.shape,train_targets.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hx4latTYlBnP"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import NearestNeighbors\n",
        "neigh = NearestNeighbors(n_neighbors=config.KNN,metric='cosine')\n",
        "neigh.fit(train_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0hOKZuc5lBnP"
      },
      "outputs": [],
      "source": [
        "test_ids = []\n",
        "test_nn_distances = []\n",
        "test_nn_idxs = []\n",
        "for filename in tqdm(test_files):\n",
        "    fold_idx = int(re.search(\"test-([0-9])\", filename)[1])\n",
        "    fold_actual_size = 2796 if fold_idx <= 5 else 2795\n",
        "    embeddings = get_embeddings(filename)[:fold_actual_size]\n",
        "    ids = get_ids(filename)[:fold_actual_size]\n",
        "    distances,idxs = neigh.kneighbors(embeddings, config.KNN, return_distance=True)\n",
        "    test_ids.append(ids)\n",
        "    test_nn_idxs.append(idxs)\n",
        "    test_nn_distances.append(distances)\n",
        "test_nn_distances = np.concatenate(test_nn_distances)\n",
        "test_nn_idxs = np.concatenate(test_nn_idxs)\n",
        "test_ids = np.concatenate(test_ids)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ds = get_test_dataset(test_files[0])\n",
        "\n",
        "# # for (sample,label) in ds:\n",
        "# #     img = sample['inp1']\n",
        "# #     plt.figure(figsize=(25,int(25*row/col)))\n",
        "# #     for j in range(row*col):\n",
        "# #         plt.subplot(row,col,j+1)\n",
        "# #         plt.title(label[j].numpy())\n",
        "# #         plt.axis('off')\n",
        "# #         plt.imshow(img[j,])\n",
        "# #     plt.show()\n",
        "# #     break\n",
        "# # print(img.shape)"
      ],
      "metadata": {
        "id": "O7fbZl1F_4zo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Sinsd0JlBnQ"
      },
      "outputs": [],
      "source": [
        "sample_submission = pd.read_csv('/content/sample_submission.csv',index_col='image')\n",
        "print(len(test_ids),len(sample_submission))\n",
        "test_df = []\n",
        "for i in tqdm(range(len(test_ids))):\n",
        "    id_ = test_ids[i]\n",
        "    targets = train_targets[test_nn_idxs[i]]\n",
        "    distances = test_nn_distances[i]\n",
        "    subset_preds = pd.DataFrame(np.stack([targets,distances],axis=1),columns=['target','distances'])\n",
        "    subset_preds['image'] = id_\n",
        "    test_df.append(subset_preds)\n",
        "test_df = pd.concat(test_df).reset_index(drop=True)\n",
        "test_df['confidence'] = 1-test_df['distances']\n",
        "test_df = test_df.groupby(['image','target']).confidence.max().reset_index()\n",
        "test_df = test_df.sort_values('confidence',ascending=False).reset_index(drop=True)\n",
        "test_df['target'] = test_df['target'].map(target_encodings)\n",
        "test_df.to_csv('test_neighbors.csv')\n",
        "test_df.image.value_counts().value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YDvfAIh9lBnQ"
      },
      "outputs": [],
      "source": [
        "sample_list = ['938b7e931166', '5bf17305f073', '7593d2aee842', '7362d7a01d00','956562ff2888']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4ayXcexlBnQ"
      },
      "outputs": [],
      "source": [
        "predictions = {}\n",
        "for i,row in tqdm(test_df.iterrows()):\n",
        "    if row.image in predictions:\n",
        "        if len(predictions[row.image])==5:\n",
        "            continue\n",
        "        predictions[row.image].append(row.target)\n",
        "    elif row.confidence>best_threshold_adjusted:\n",
        "        predictions[row.image] = [row.target,'new_individual']\n",
        "    else:\n",
        "        predictions[row.image] = ['new_individual',row.target]\n",
        "        \n",
        "for x in tqdm(predictions):\n",
        "    if len(predictions[x])<5:\n",
        "        remaining = [y for y in sample_list if y not in predictions]\n",
        "        predictions[x] = predictions[x]+remaining\n",
        "        predictions[x] = predictions[x][:5]\n",
        "    predictions[x] = ' '.join(predictions[x])\n",
        "    \n",
        "predictions = pd.Series(predictions).reset_index()\n",
        "predictions.columns = ['image','predictions']\n",
        "predictions.to_csv('submission.csv',index=False)\n",
        "predictions.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DBjK0JzKTb27"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OyHikdNKdnDQ"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [],
      "name": "happywhale-hybrid-effnet-swin-detic-crop.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}